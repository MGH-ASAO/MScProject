# -*- coding: utf-8 -*-
"""Normalizing_flows_toy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14AKRk6PLO003IAV1XP4rC6RWKDcYsSoW
"""



"""## Normalizing flows - simple example of generating digits"""

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt

# Define the Normalizing Flow model (from the provided code)
class NormalizingFlow(nn.Module):
    def __init__(self, dim, n_flows=8):
        super(NormalizingFlow, self).__init__()
        self.dim = dim
        self.flows = nn.ModuleList([AffineCouplingLayer(dim) for _ in range(n_flows)])
        self.batch_norms = nn.ModuleList([nn.BatchNorm1d(dim) for _ in range(n_flows)])
        self.permute = Permute(dim)

    def forward(self, x):
        log_det_sum = 0
        for flow, bn in zip(self.flows, self.batch_norms):
            x, log_det = flow(x)
            log_det_sum += log_det
            x = bn(x)
            x = self.permute(x)
        return x, log_det_sum

    def inverse(self, z):
        for flow, bn in zip(reversed(self.flows), reversed(self.batch_norms)):
            z = self.permute.inverse(z)
            z = bn.inverse(z)
            z = flow.inverse(z)
        return z

class AffineCouplingLayer(nn.Module):
    def __init__(self, dim):
        super(AffineCouplingLayer, self).__init__()
        self.dim = dim
        self.half_dim = dim // 2
        hidden_dim = dim * 2

        self.net = nn.Sequential(
            nn.Linear(self.half_dim, hidden_dim),
            nn.LeakyReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LeakyReLU(),
            nn.Linear(hidden_dim, self.half_dim),  # Only predict the shift
        )

        nn.init.zeros_(self.net[-1].weight)
        nn.init.zeros_(self.net[-1].bias)

    def forward(self, x):
        x1, x2 = x[:, :self.half_dim], x[:, self.half_dim:]
        shift = self.net(x2)
        scale = torch.ones_like(shift)  # Volume-preserving, scale = 1
        y1 = x1 * scale + shift
        y2 = x2
        log_det = torch.zeros(x.size(0), device=x.device)  # log_det is 0 for volume-preserving
        return torch.cat([y1, y2], dim=1), log_det

    def inverse(self, y):
        y1, y2 = y[:, :self.half_dim], y[:, self.half_dim:]
        h = self.net(y2)
        shift = h[:, :self.half_dim]
        scale = torch.tanh(h[:, self.half_dim:]) * 0.5 + 1
        x1 = (y1 - shift) / (scale + 1e-8)
        x2 = y2
        return torch.cat([x1, x2], dim=1)

class Permute(nn.Module):
    def __init__(self, dim):
        super(Permute, self).__init__()
        self.register_buffer('p', torch.randperm(dim))
        self.register_buffer('inv_p', torch.argsort(self.p))

    def forward(self, x):
        return x[:, self.p]

    def inverse(self, x):
        return x[:, self.inv_p]

def initialize_weights(m):
    if isinstance(m, nn.Linear):
        nn.init.xavier_uniform_(m.weight, gain=0.01)
        nn.init.zeros_(m.bias)

# Define the training loop and utilities
def train_flow(flow, data_loader, optimizer, device):
    flow.train()
    total_loss = 0
    for batch, _ in data_loader:
        batch = batch.to(device)
        batch = batch.view(batch.size(0), -1)  # Flatten the images

        optimizer.zero_grad()
        z, log_det = flow(batch)
        log_pz = -0.5 * torch.sum(z ** 2, dim=1)
        loss = -torch.mean(log_pz + log_det)

        loss.backward()
        optimizer.step()

        total_loss += loss.item()
    return total_loss / len(data_loader)

def generate_samples(flow, num_samples, device):
    flow.eval()
    with torch.no_grad():
        z = torch.randn(num_samples, flow.dim).to(device)
        samples = flow.inverse(z)
        samples = samples.view(-1, 1, 28, 28)
    return samples.cpu()

"""# Train the flow on MNIST"""

# Load the MNIST dataset
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
train_loader = DataLoader(mnist_train, batch_size=128, shuffle=True)

# Instantiate the model, optimizer, and device
device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
flow = flow = NormalizingFlow(dim=28*28, n_flows=16).to(device)
flow.apply(initialize_weights)

optimizer = optim.Adam(flow.parameters(), lr=1e-3)

# Train the model
n_epochs = 10
for epoch in range(1, n_epochs + 1):
    loss = train_flow(flow, train_loader, optimizer, device)
    print(f'Epoch {epoch}, Loss: {loss:.4f}')

# Generate and visualize some samples
samples = generate_samples(flow, 16, device)

# Plot generated samples
fig, axes = plt.subplots(4, 4, figsize=(4, 4))
for i, ax in enumerate(axes.flatten()):
    ax.imshow(samples[i].squeeze(), cmap='gray')
    ax.axis('off')
plt.show()

"""# Downloading issues
If you get an issue downloading MNIST directly from the original directory, try removing MNIST from the current cache and download mannualy.
"""



from torchvision.datasets import MNIST
from torchvision import transforms
from torch.utils.data import DataLoader

# Use torchvision datasets with a different download URL
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
mnist_train = MNIST(root='./data', train=True, download=True, transform=transform)

# DataLoader
train_loader = DataLoader(mnist_train, batch_size=128, shuffle=True)